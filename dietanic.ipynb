{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EDA To Prediction (DieTanic)\n* Titanic dataset은 데이터 사이언스를 시작하는 초심자들이나 캐글 대회에 참여하려는 초보자들에게 매우 좋은 데이터셋이다. \n* 이 notebook의 목적은 예측 모델링 문제의 workflow가 어떤 것인지 아이디어를 제공하는 것이다. feature들을 확인하는 방법, 새로운 feature들과 머신러닝 컨셉을 추가하는 방법. 이 notebook은 초보자들이 모든 단계를 이해할 수 있도록 최대한 기본적으로 기록하였다.\n---\n## Contents of the Notebook:\n### Part1 : Exploratory Data Analysis(EDA)\n1. feature 분석\n2. 여러 feature들을 고려해서 어떤 관계성이나 흐름을 파악한다.\n\n### Part2 : Feature Engineering and Data Cleaning\n1. 새로운 feature 추가\n2. 불필요한 feature들 제거\n3. 모델링에 적합한 feature로 변환하기\n\n### Part3 : Predictive Modeling\n1. Basic Modeling 돌리기\n2. Cross Validation\n3. Ensembling\n4. Important Features Extraction","metadata":{}},{"cell_type":"markdown","source":"## Part1 : Exploratory Data Analysis(EDA)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:11.834860Z","iopub.execute_input":"2022-02-03T12:43:11.835453Z","iopub.status.idle":"2022-02-03T12:43:12.921050Z","shell.execute_reply.started":"2022-02-03T12:43:11.835349Z","shell.execute_reply":"2022-02-03T12:43:12.920089Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/titanic/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:12.922435Z","iopub.execute_input":"2022-02-03T12:43:12.922683Z","iopub.status.idle":"2022-02-03T12:43:12.945135Z","shell.execute_reply.started":"2022-02-03T12:43:12.922654Z","shell.execute_reply":"2022-02-03T12:43:12.944390Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:12.946498Z","iopub.execute_input":"2022-02-03T12:43:12.946992Z","iopub.status.idle":"2022-02-03T12:43:12.972802Z","shell.execute_reply.started":"2022-02-03T12:43:12.946949Z","shell.execute_reply":"2022-02-03T12:43:12.972012Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum() # checking for total null values","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:12.974416Z","iopub.execute_input":"2022-02-03T12:43:12.974641Z","iopub.status.idle":"2022-02-03T12:43:12.983175Z","shell.execute_reply.started":"2022-02-03T12:43:12.974604Z","shell.execute_reply":"2022-02-03T12:43:12.982436Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Age, Cabin and Embarked은 결측값이 존재한다. 이것들을 수정해보자.","metadata":{}},{"cell_type":"markdown","source":"### How many Survived??","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(1,2, figsize=(18,8))\ndata['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%', ax=ax[0], shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived', data=data, ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:12.985238Z","iopub.execute_input":"2022-02-03T12:43:12.985962Z","iopub.status.idle":"2022-02-03T12:43:13.329269Z","shell.execute_reply.started":"2022-02-03T12:43:12.985920Z","shell.execute_reply":"2022-02-03T12:43:13.328421Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"* 이 사고에서 살아남은 승객이 얼마 없다는 것이 명백해보인다.\n* training set에 총 891명 중 오직 350명만 살아남았다. 훈련 데이터셋만 봤을때 이 충돌사고에서 오직 38.4%만 살아남은 것이다. 우리는 이 데이터로부터 더 좋은 인사이트를 얻기 위해서 그리고 어떤 범주의 승객들이 생존하고 사망했는지 알기위해서 더 파헤쳐봐야한다.\n* 데이터셋의 다양한 feature들을 가지고 생존율을 확인해볼것이다.그 feature는 Sex, Port of Embarcation, Age 등 일것이다.\n\n#### 먼저 다양한 타입의 feature들을 이해해보자!\n---\n## Types of Features\n#### Categorical Features(범주형 특성):\n범주형 변수는 두개 이상의 범주를 가진 데이터이고, 각각의 값들이 그 범주로 분류된다. 예를 들어, 성별은 두 개(남성과 여성)의 범주를 가진 범주형 변수이다. 이 상태로는 이러한 범주형 변수들을 정렬하거나 순서를 매기기 어렵다. 이런 변수들은 명목변수라고도 한다.\n#### 범주형 features : Sex, Embarked\n\n#### Ordinal Features(순서형 특성):\n순서형 특성은 범주형 데이터들과 비슷한데, 차이점은 상대적인 순서나 값들을 정렬해줄 수 있다. 예를 들어, 크다, 중간, 작다라는 값들을 가지는 키와 같은 특성이 있을때, 키는 순서형 변수다. 여기서 우리는 변수의 상대적인 순서를 가진다.\n#### 순서형 features : PClass\n\n#### Continuous Feature(연속형 특성):\n이 특성은 특성 컬럼에서 어떤 두 지점 사이의 값이거나 최소와 최대 사이의 값을 가질때 연속적이라고 할 수 있다. \n#### 연속형 features : Age\n---\n## Analysing the Features(특성 분석)\n### Sex -> Categorical Feature","metadata":{}},{"cell_type":"code","source":"data.groupby(['Sex', 'Survived'])['Survived'].count()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:13.330413Z","iopub.execute_input":"2022-02-03T12:43:13.330665Z","iopub.status.idle":"2022-02-03T12:43:13.342054Z","shell.execute_reply.started":"2022-02-03T12:43:13.330612Z","shell.execute_reply":"2022-02-03T12:43:13.340918Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1,2, figsize=(18,8))\ndata[['Sex', 'Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survivied vs Sex')\nsns.countplot('Sex', hue='Survived', data=data, ax=ax[1])\nax[1].set_title('Sex : Survived vs Dead')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:13.343932Z","iopub.execute_input":"2022-02-03T12:43:13.344399Z","iopub.status.idle":"2022-02-03T12:43:13.673273Z","shell.execute_reply.started":"2022-02-03T12:43:13.344354Z","shell.execute_reply":"2022-02-03T12:43:13.672429Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"* 흥미로운 결과가 나왔다. 남자 승객수가 여자 승객수 보다 훨씬 많다. 그런데 생존자수는 여자가 남자의 거의 2배다. 남자 승객의 생존율이 약 18-19%인 반면에 여자 승객의 생존율은 대략 75%이다. \n* 여기서 Sex는 모델링하는데 매우 중요한 feature라는 걸 알 수 있다.\n---\n### Pclass -> Ordinal Feature","metadata":{}},{"cell_type":"code","source":"pd.crosstab(data.Pclass, data.Survived, margins=True).style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:13.674389Z","iopub.execute_input":"2022-02-03T12:43:13.674616Z","iopub.status.idle":"2022-02-03T12:43:13.780501Z","shell.execute_reply.started":"2022-02-03T12:43:13.674589Z","shell.execute_reply":"2022-02-03T12:43:13.779890Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1,2, figsize=(18,8))\ndata['Pclass'].value_counts().plot.bar(color=['#CD7F32', '#FFDF00', '#D3D3D3'], ax=ax[0])\nax[0].set_title('Number of Passengers By Pclass')\nax[0].set_ylabel('Count')\nsns.countplot('Pclass', hue='Survived', data=data, ax=ax[1])\nax[1].set_title('Pclass : Survived vs Dead')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:13.781412Z","iopub.execute_input":"2022-02-03T12:43:13.781789Z","iopub.status.idle":"2022-02-03T12:43:14.109740Z","shell.execute_reply.started":"2022-02-03T12:43:13.781746Z","shell.execute_reply":"2022-02-03T12:43:14.108772Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"* 사람들은 흔히 '돈으로 모든걸 살 수 없다'고 말한다. 그런데 우리는 여기서 Pclass 1의 승객들이 매우 높은 우선순위로 구조된것을 명백하게 알 수 있다. 물론 Pclass 3의 승객수가 훨씬 많긴하지만, 그래도 Pclass 3의 생존율은 고작 25%로 현저히 낮다.\n* Pclass 1의 생존율은 약 63%이고 Pclass 2의 생존율은 약 48%인 것과 비교해서. 그래서 돈과 지위는 중요하다. 물질만능주의 세계란...\n* 이제 좀 더 흥미로운 관찰을 해보자. Sex와 Pclass를 동시에 고려하여 생존율을 확인해보자!","metadata":{}},{"cell_type":"code","source":"pd.crosstab([data.Sex, data.Survived], data.Pclass, margins=True).style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:14.112186Z","iopub.execute_input":"2022-02-03T12:43:14.112438Z","iopub.status.idle":"2022-02-03T12:43:14.166281Z","shell.execute_reply.started":"2022-02-03T12:43:14.112408Z","shell.execute_reply":"2022-02-03T12:43:14.165282Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"sns.factorplot('Pclass', 'Survived', hue='Sex', data=data)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:14.167485Z","iopub.execute_input":"2022-02-03T12:43:14.167740Z","iopub.status.idle":"2022-02-03T12:43:14.677420Z","shell.execute_reply.started":"2022-02-03T12:43:14.167710Z","shell.execute_reply":"2022-02-03T12:43:14.676477Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"* 우리는 여기서 FactorPlot을 사용하였는데, 그 이유는 이 플랏이 범주형 데이터를 잘 구분해주기 때문이다.\n* CrossTab이랑 FactorPlot을 보면, Pclass1의 여자 생존율은 거의 95-96% 이고, Pclass1에서 여자 승객 총 94명 중 오직 3명만 사망한 것을 쉽게 알 수 있다.\n* 또한 Pclass와 상관없이, 구조 당시 여성이 우선순위였다는 것이 명백하다.\n* 이것을 통해 Pclass 또한 중요한 특성인 것을 알 수 있다. 다른 특성들도 분석해보자!\n---\n### Age -> Continuous Feature","metadata":{}},{"cell_type":"code","source":"print('Oldest Passenger was of : ', data['Age'].max(), 'Years')\nprint('Youngest Passenger was of : ', data['Age'].min(), 'Years')\nprint('Average Age on the ship : ', data['Age'].mean(), 'Years')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:14.678684Z","iopub.execute_input":"2022-02-03T12:43:14.678895Z","iopub.status.idle":"2022-02-03T12:43:14.686499Z","shell.execute_reply.started":"2022-02-03T12:43:14.678863Z","shell.execute_reply":"2022-02-03T12:43:14.685698Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1,2, figsize=(18,8))\nsns.violinplot('Pclass', 'Age', hue='Survived', data=data, split=True\n               , ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0, 110, 10))\nsns.violinplot('Sex', 'Age', hue='Survived', data=data, split=True, ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0, 110, 10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:14.687738Z","iopub.execute_input":"2022-02-03T12:43:14.688554Z","iopub.status.idle":"2022-02-03T12:43:15.165263Z","shell.execute_reply.started":"2022-02-03T12:43:14.688510Z","shell.execute_reply":"2022-02-03T12:43:15.164597Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"##### Observaions:\n1) 아이들의 수는 Pclass가 높을수록(3으로 갈수록) 증가하고, 10살이하 승객들의 생존율은 Pclass와 상관없어 보인다.  \n2) Pclass1에서 20-50대의 생존율은 매우 높고 심지어 여성의 생존율보다 더 높다.  \n3) 남섬의 경우, 나이가 많을수록 생존율이 줄어든다.  \n\n* 위에서도 봤듯이, Age feature는 177개의 결측값을 가지고 있다. 이 결측치들을 채우기 위해서 age 데이터셋의 평균값을 사용할 수 있다.\n* 그러나 문제는 나이층이 매우 광범위하고 넓게 퍼져있다. 4살 아이의 나이를 평균나이인 29살로 대체할 수는 없다. 어떤 연령대가 속하는지 알 수 있는 방법이 있을까?\n* 그렇다! 방법은 바로 Name feature를 확인해보는 것이다. 이름 데이터들을 보면, Mr나 Mrs와 같은 인삿말을 볼 수 있다. 따라서 Mr과 Mrs의 평균나이를 관련있는 그룹에 매칭해주면 된다.\n---\n### \"What is in a Name\" -> Feature :p","metadata":{}},{"cell_type":"code","source":"data['Initial'] = 0\nfor i in data:\n    data['Initial'] = data.Name.str.extract('([A-Za-z]+)\\.')\n# 인삿말을 추출해보자!","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:15.166603Z","iopub.execute_input":"2022-02-03T12:43:15.167077Z","iopub.status.idle":"2022-02-03T12:43:15.204087Z","shell.execute_reply.started":"2022-02-03T12:43:15.167037Z","shell.execute_reply":"2022-02-03T12:43:15.203294Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"* 여기서 우리는 Regex(정규표현식):[A-za-z]+)...를 사용할 것이다. 이게 뭐냐면, A-z or a-z와 마지막에 .(dot)으로 표기된 문자열을 찾는 것이다. 이것을 통해 우리는 Name으로부터 Initial을 성공적으로 추출할 수 있다.","metadata":{}},{"cell_type":"code","source":"pd.crosstab(data.Initial, data.Sex).T.style.background_gradient(cmap='summer_r')\n# Initial과 Sex를 확인해보자!","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:15.205249Z","iopub.execute_input":"2022-02-03T12:43:15.206082Z","iopub.status.idle":"2022-02-03T12:43:15.256072Z","shell.execute_reply.started":"2022-02-03T12:43:15.206045Z","shell.execute_reply":"2022-02-03T12:43:15.255531Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"* 여기서 Mlle이나 Mme과 같이 Miss를 나타내는데 잘 못 표기된 Initial들이 있다. 이 데이터들은 Miss로 바꿔주고 다른 값들도 이런식으로 바꿔준다.","metadata":{}},{"cell_type":"code","source":"data['Initial'].replace(['Mlle', 'Mme', 'Ms', 'Dr', 'Major', 'Lady', 'Countess', 'Jonkheer', 'Col', 'Rev', 'Capt', 'Sir', 'Don'],\n                       ['Miss', 'Miss', 'Miss', 'Mr', 'Mr', 'Mrs', 'Mrs', 'Other', 'Other', 'Other', 'Mr', 'Mr', 'Mr'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:15.257058Z","iopub.execute_input":"2022-02-03T12:43:15.257608Z","iopub.status.idle":"2022-02-03T12:43:15.263670Z","shell.execute_reply.started":"2022-02-03T12:43:15.257577Z","shell.execute_reply":"2022-02-03T12:43:15.263020Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"data.groupby('Initial')['Age'].mean()\n# Initial에 따른 평균 나이를 확인해보자!","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:15.264876Z","iopub.execute_input":"2022-02-03T12:43:15.265229Z","iopub.status.idle":"2022-02-03T12:43:15.276477Z","shell.execute_reply.started":"2022-02-03T12:43:15.265192Z","shell.execute_reply":"2022-02-03T12:43:15.275608Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Age의 결측치를 채워보자","metadata":{}},{"cell_type":"code","source":"# 결측치들을 올림한 평균나이로 채워주자\ndata.loc[(data.Age.isnull())&(data.Initial=='Mr'), 'Age'] = 33\ndata.loc[(data.Age.isnull())&(data.Initial=='Mrs'), 'Age'] = 36\ndata.loc[(data.Age.isnull())&(data.Initial=='Master'), 'Age'] = 5\ndata.loc[(data.Age.isnull())&(data.Initial=='Miss'), 'Age'] = 22\ndata.loc[(data.Age.isnull())&(data.Initial=='Other'), 'Age'] = 46","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:15.277974Z","iopub.execute_input":"2022-02-03T12:43:15.278251Z","iopub.status.idle":"2022-02-03T12:43:15.293437Z","shell.execute_reply.started":"2022-02-03T12:43:15.278222Z","shell.execute_reply":"2022-02-03T12:43:15.292606Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"data.Age.isnull().any()\n# 이제 Age에 null값이 없을 것이다!","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:15.294676Z","iopub.execute_input":"2022-02-03T12:43:15.295193Z","iopub.status.idle":"2022-02-03T12:43:15.306782Z","shell.execute_reply.started":"2022-02-03T12:43:15.295148Z","shell.execute_reply":"2022-02-03T12:43:15.305954Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1,2, figsize=(20,10))\ndata[data['Survived']==0].Age.plot.hist(ax=ax[0], bins=20, edgecolor='black', color='red')\nax[0].set_title('Survived = 0')\nx1 = list(range(0, 85, 5))\nax[0].set_xticks(x1)\ndata[data['Survived']==1].Age.plot.hist(ax=ax[1], color='green', bins=20, edgecolor='black')\nax[1].set_title('Survived = 1')\nx2 = list(range(0, 85, 5))\nax[1].set_xticks(x2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:15.307936Z","iopub.execute_input":"2022-02-03T12:43:15.308928Z","iopub.status.idle":"2022-02-03T12:43:15.862220Z","shell.execute_reply.started":"2022-02-03T12:43:15.308886Z","shell.execute_reply":"2022-02-03T12:43:15.861398Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"##### Observations:\n1) 영유아(5세미만)들은 상당수 구조되었다.(여성과 아동 우선 정책)  \n2) 최고령자(80세)도 구조되었다.  \n3) 사망자수가 가장 작은 그룹은 30-40대이다.  ","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Pclass', 'Survived', col='Initial', data=data)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:15.863475Z","iopub.execute_input":"2022-02-03T12:43:15.864217Z","iopub.status.idle":"2022-02-03T12:43:16.842898Z","shell.execute_reply.started":"2022-02-03T12:43:15.864169Z","shell.execute_reply":"2022-02-03T12:43:16.842078Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"* 여성과 아동 우선 정책은 Pclass와 상관없이 유효하다.\n---\n### Embarked -> Categorical Value","metadata":{}},{"cell_type":"code","source":"pd.crosstab([data.Embarked, data.Pclass], [data.Sex, data.Survived], margins=True).style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:16.844110Z","iopub.execute_input":"2022-02-03T12:43:16.844694Z","iopub.status.idle":"2022-02-03T12:43:16.913702Z","shell.execute_reply.started":"2022-02-03T12:43:16.844653Z","shell.execute_reply":"2022-02-03T12:43:16.912715Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"#### Chances for Survival by Port of Embarkation","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Embarked', 'Survived', data=data)\nfig = plt.gcf()\nfig.set_size_inches(5,3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:16.915208Z","iopub.execute_input":"2022-02-03T12:43:16.915580Z","iopub.status.idle":"2022-02-03T12:43:17.153393Z","shell.execute_reply.started":"2022-02-03T12:43:16.915538Z","shell.execute_reply":"2022-02-03T12:43:17.152414Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"* Port C에서의 생존율이 약 0.55로 가장 높고 S에서 가장 낮다.","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(2,2, figsize=(20,15))\nsns.countplot('Embarked', data=data, ax=ax[0,0])\nax[0,0].set_title('No. of Passengers Boarded')\nsns.countplot('Embarked', hue='Sex', data=data, ax=ax[0,1])\nax[0,1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked', hue='Survived', data=data, ax=ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked', hue='Pclass', data=data, ax=ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:17.154744Z","iopub.execute_input":"2022-02-03T12:43:17.155096Z","iopub.status.idle":"2022-02-03T12:43:17.730533Z","shell.execute_reply.started":"2022-02-03T12:43:17.155057Z","shell.execute_reply":"2022-02-03T12:43:17.729518Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"#### Observations:\n1) S에서 탑승한 최다 승객수의 대다수가 Pclass 3에 속한다.   \n2) C의 승객 중 상당수가 살아남았기 때문에 운이 좋아 보인다. 그 이유는 구조된 이들이 모두 Pclass1과 Pclass2이기 때문일 것이다.  \n3) S 항구에서는 대다수 부자들이 탑승한 것 같다. 그럼에도 여전히 이 항구에서 탑승한 승객들의 생존율은 낮다, 그 이유는 Pclass3의 많은 승객들 대략 81%가 구조되지 못했기 때문이다.  \n4) Q 항구의 승객들은 약 95%가 Pclass3이다. ","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Pclass', 'Survived', hue='Sex', col='Embarked', data=data)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:17.732197Z","iopub.execute_input":"2022-02-03T12:43:17.732501Z","iopub.status.idle":"2022-02-03T12:43:18.783725Z","shell.execute_reply.started":"2022-02-03T12:43:17.732459Z","shell.execute_reply":"2022-02-03T12:43:18.782822Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"#### Observations:\n1) Pclass와 관계없이 Pclass1과 Pclass2의 여성 생존율은 거의 1이다.  \n2) S 항구에서 탑승한 Pclass3의 남성과 여성 승객 모두 생존율이 매우 낮은 걸로 보아 매우 불행한 것 같다.(돈이 중요하다....)   \n3) Q 항구에서 탑승한 남성의 경우 거의 모든이들이 Pclass3이기에 매우 불행한 결론을 보인다.  \n---\n### Filling Embarked NaN\n* 승객의 대다수가 S 항구에서 탑승했으므로, 우리는 Embarked NaN값을 S로 채워주자. ","metadata":{}},{"cell_type":"code","source":"data['Embarked'].fillna('S', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:18.785315Z","iopub.execute_input":"2022-02-03T12:43:18.785663Z","iopub.status.idle":"2022-02-03T12:43:18.792049Z","shell.execute_reply.started":"2022-02-03T12:43:18.785575Z","shell.execute_reply":"2022-02-03T12:43:18.791038Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"data.Embarked.isnull().any()\n# 드디어 NaN 값을 없앴다!","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:18.793745Z","iopub.execute_input":"2022-02-03T12:43:18.793994Z","iopub.status.idle":"2022-02-03T12:43:18.804854Z","shell.execute_reply.started":"2022-02-03T12:43:18.793964Z","shell.execute_reply":"2022-02-03T12:43:18.803998Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### SibSp -> Discrete Feature\n* 이 특성은 해당 승객이 혼자인지 그의 가족구성원과 함께인지를 알려준다.\n* Sibling = brother, sister, stepbrother, stepsister\n* Spouse = husband, wife","metadata":{}},{"cell_type":"code","source":"pd.crosstab([data.SibSp], data.Survived).style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:18.810696Z","iopub.execute_input":"2022-02-03T12:43:18.811117Z","iopub.status.idle":"2022-02-03T12:43:18.836063Z","shell.execute_reply.started":"2022-02-03T12:43:18.811076Z","shell.execute_reply":"2022-02-03T12:43:18.834714Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot('SibSp', 'Survived', data=data, ax=ax[0])\nax[0].set_title('SibSp vs Survived')\nsns.pointplot('SibSp', 'Survived', data=data, ax=ax[1])\nax[1].set_title('SipSb vs Survived')\nplt.close(2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:18.837132Z","iopub.execute_input":"2022-02-03T12:43:18.837447Z","iopub.status.idle":"2022-02-03T12:43:19.501526Z","shell.execute_reply.started":"2022-02-03T12:43:18.837422Z","shell.execute_reply":"2022-02-03T12:43:19.500597Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(data.SibSp, data.Pclass).style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:19.502848Z","iopub.execute_input":"2022-02-03T12:43:19.503071Z","iopub.status.idle":"2022-02-03T12:43:19.531058Z","shell.execute_reply.started":"2022-02-03T12:43:19.503044Z","shell.execute_reply":"2022-02-03T12:43:19.530187Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"#### Observations:\n* barplot과 pointplot은 해당승객이 형제자매 없이 혼자 탑승한 경우라면 그의 생존율은 34.5%이라는 것을 알려준다. 형제자매 수가 증가하면 그래프는 대체적으로 감소한다. 이건 당연하다. 왜냐하면 만약 내 가족이 같이 탑승해 있다면, 내 목숨보다 가족의 목숨을 우선시했을테니까. 놀랍게도 5-8명의 가족구성원의 경우 생존율이 0%이다. 아마 이유는 Pclass와 관련있지 않을까?\n* 그 이유는 Pclass가 맞다. 위의 crosstab을 보면, SibSp>3인 승객의 경우 모두 Pclass가 3이다. 이것은 Pclass3에 속한 모든 대가족(4인이상)이 사망하는 것에 임박하다는 뜻이다.\n---\n### Parch","metadata":{}},{"cell_type":"code","source":"pd.crosstab(data.Parch, data.Pclass).style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:19.532286Z","iopub.execute_input":"2022-02-03T12:43:19.532594Z","iopub.status.idle":"2022-02-03T12:43:19.560454Z","shell.execute_reply.started":"2022-02-03T12:43:19.532554Z","shell.execute_reply":"2022-02-03T12:43:19.559702Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"* crosstab은 여기서 한번 더 대가족의 경우 Pclass3인걸 알려준다.","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot('Parch', 'Survived', data=data, ax=ax[0])\nax[0].set_title('Parch vs Survived')\nsns.pointplot('Parch', 'Survived', data=data, ax=ax[1])\nax[1].set_title('Parch vs Survived')\nplt.close(2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:19.561419Z","iopub.execute_input":"2022-02-03T12:43:19.561604Z","iopub.status.idle":"2022-02-03T12:43:20.174110Z","shell.execute_reply.started":"2022-02-03T12:43:19.561581Z","shell.execute_reply":"2022-02-03T12:43:20.173362Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"#### Observations:\n* 여기서도 결과가 비슷하다. 부모와 동승한 승객의 경우 생존율이 높다. 그렇지만 구성원 수가 증가할수록 생존율은 감소한다.\n* 부모가 1-3명 정도인 경우는 생존율이 높다. 그러나 혼자인 경우도 치명적이고 부모가 4명 초과인 경우도 생존율이 감소한다.\n---\n### Fare -> Continuous Feature","metadata":{}},{"cell_type":"code","source":"print('Highest Fare was:', data['Fare'].max())\nprint('Lowest Fare was:', data['Fare'].min())\nprint('Average Fare was:', data['Fare'].mean())","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:20.175526Z","iopub.execute_input":"2022-02-03T12:43:20.175985Z","iopub.status.idle":"2022-02-03T12:43:20.183860Z","shell.execute_reply.started":"2022-02-03T12:43:20.175945Z","shell.execute_reply":"2022-02-03T12:43:20.182972Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1,3, figsize=(20,8))\nsns.distplot(data[data['Pclass']==1].Fare, ax=ax[0])\nax[0].set_title('Fares in Pclass 1')\nsns.distplot(data[data['Pclass']==2].Fare, ax=ax[1])\nax[1].set_title('Fares in Pclass 2')\nsns.distplot(data[data['Pclass']==3].Fare, ax=ax[2])\nax[2].set_title('Fares in Pclass 3')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:20.185409Z","iopub.execute_input":"2022-02-03T12:43:20.185657Z","iopub.status.idle":"2022-02-03T12:43:20.930223Z","shell.execute_reply.started":"2022-02-03T12:43:20.185606Z","shell.execute_reply":"2022-02-03T12:43:20.929311Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"* Pclass1에서 승객들의 요금 차이가 크다. 이 분포는 표준이 감소함에 따라 계속 감소한다. 이것도 연속형 자료이므로 구간화를 사용하여 이산값으로 변환할 수 있다. \n---\n#### Observations in a Nutshell for all features:\n* Sex : 남성과 비교해서 여성의 생존율이 높다.\n* Pclass : 1등석 승객이 되면 생존 가능성이 더 높아진다는 가시적인 추세가 있다. 3등석의 생존율은 매우 낮다. 여성의 경우 1등석의 생존확률은 거의 1이고 2등석의 생존율도 높다. 돈은 승리한다!!!\n* Age : 5-10세의 어린이는 생존 가능성이 높다. 15-35세 사이의 승객이 많이 사망했다.\n* Embarked : 매우 흥미로운 특성이다. 1등석 승객의 대다수가 S에서 탑승했음에도 불구하고 C에서 생존할 가능성이 더 좋아 보인다. Q의 승객은 모두 3등석이다.\n* Parch+SibSp : 형제자매가 1-2명, 배우자가 있거나 부모가 1-3명인 경우 혼자이거나 대가족이 함께 여행하는 것보다 생존 가능성이 더 높다.\n___\n### Correlation Between The Features","metadata":{}},{"cell_type":"code","source":"sns.heatmap(data.corr(), annot=True, cmap='RdYlGn', linewidth=0.2)\n# data.corr() -> correaltion matrix\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:20.931364Z","iopub.execute_input":"2022-02-03T12:43:20.931578Z","iopub.status.idle":"2022-02-03T12:43:21.388663Z","shell.execute_reply.started":"2022-02-03T12:43:20.931553Z","shell.execute_reply":"2022-02-03T12:43:21.387822Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"#### Heatmap 해석\n* 가장 먼저 주목해야 할 것은 알파벳이나 문자열 사이의 상관관계를 알 수 엉ㅂㅅ다는 것이 분명하기 때문에 숫자 데이터만 비교된다는 것이다. 플롯을 이해하기 전에 상관관계가 정확히 무엇인지 살펴보겠습니다.   \n* Positive Correlation : 특성A의 증가가 특성 B의 증가로 이어진다면 양의 상관관계가 있다. 값1은 완전한 양의 상관관계를 의미한다.\n* Negative Correlation : 특성A가 증가할때 특성B가 감소하면 음의 상관관계가 있다. 값 -1은 완전히 음의 상관관계를 의미한다.\n* 이제 두 특성이 크게 또는 완전히 상관관계가 있으므로 하나의 증가가 다른 특성의 증가로 이어진다고 가정해 보겠습니다. 이는 두 기능 모두 매우 유사한 정보를 포함하고 있으며 정보의 변동이 거의 혹은 전혀 없음을 의미한다. 둘 다 거의 동일한 정보를 포함하므로 이를 다중공선성(MultiColinearity)이라고 한다.\n* 둘 중 하나가 중복되므로 둘 다 사용해야 한다고 생각하나요? 모델을 만들거나 훈련하는 동안 훈련 시간과 많은 이점을 줄이므로 중복 특성을 제거하려고 해야한다.\n* 이제 위의 히트맵에서 특성이 그다지 상관관계가 없음을 알 수 있다. 가장 높은 상관관계는 SibSp와 Parch 사이, 즉 0.41이다. 따라서 모든 특성을 계속 사용할 수 있다. \n___\n## Part2 : Feature Engineering and Data Cleaning\n* Feature Engineering 이란?\n* 특성이 있는 데이터 세트가 제공될 때마다 모든 특성이 중요할 필요는 없다. 제거해야할 중복 특성이 많이 있을 수 있다. 또한 다른 특성에서 정보를 관찰하거나 추출하여 새로운 기능을 얻거나 추가할 수 있다.\n* Name 특성을 사용하여 Initial 특성을 가져오는 것은 한 에시다. 새로운 특성을 얻고 몇가지를 제거할 수 있는지 보자. 또한 기존 관련 특성을 Predicitve Modeling에 적합한 형식으로 변환할 것이다.\n---\n### Age_band\n* Age Feature의 문제점:\n* 나이는 연속적인 특성이라고 앞서 언급했듯이 기계 학습 모델의 연속 변수에는 문제가 있다. 예) 스포츠인을 성별로 그룹화하거나 정렬하면 쉽게 남성과 여성을 구분할 수 있다.\n* 이제 연령별로 그룹화하라고 하면 어떻게 해야할까? 30명의 사람이 있는 경우 30개의 연령 값이 있을 수 있다. 이제 이것이 문제가 된다.\n* 그룹화와 정규화를 통해 이러한 연속값을 범주형값으로 변환해야 한다. 여기서는 그룹화를 사용할 것이다. 즉, 연령대 범위를 단일 그룹으로 그룹화하거나 단일값을 할당할 것이다.\n* 승객의 최대 연령은 80세였다. 따라서 0-80의 범위를 5개의 그룹으로 나눈다. 80/5=16이다. 크기가 16인 그룹.","metadata":{}},{"cell_type":"code","source":"data['Age_band']=0\ndata.loc[data['Age']<=16, 'Age_band']=0\ndata.loc[(data['Age']>16)&(data['Age']<=32), 'Age_band']=1\ndata.loc[(data['Age']>32)&(data['Age']<=48), 'Age_band']=2\ndata.loc[(data['Age']>48)&(data['Age']<=64), 'Age_band']=3\ndata.loc[data['Age']>64, 'Age_band']=4\ndata.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:21.390137Z","iopub.execute_input":"2022-02-03T12:43:21.390611Z","iopub.status.idle":"2022-02-03T12:43:21.415492Z","shell.execute_reply.started":"2022-02-03T12:43:21.390568Z","shell.execute_reply":"2022-02-03T12:43:21.414648Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"data['Age_band'].value_counts().to_frame().style.background_gradient(cmap='summer')\n# 각 그룹의 수를 확인해보자","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:21.416781Z","iopub.execute_input":"2022-02-03T12:43:21.417262Z","iopub.status.idle":"2022-02-03T12:43:21.433500Z","shell.execute_reply.started":"2022-02-03T12:43:21.417219Z","shell.execute_reply":"2022-02-03T12:43:21.432703Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"sns.factorplot('Age_band','Survived', data=data, col='Pclass')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:21.434746Z","iopub.execute_input":"2022-02-03T12:43:21.435367Z","iopub.status.idle":"2022-02-03T12:43:22.267606Z","shell.execute_reply.started":"2022-02-03T12:43:21.435329Z","shell.execute_reply":"2022-02-03T12:43:22.266717Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"* Pclass와 상관없이 나이가 증가할수록 생존율이 떨어진다.\n---\n### Family_Size and Alone\n* 여기서 우리는 'Family_size'와 'Alone'이라는 새로운 특성을 만들고 분석할 것이다. 이 특성은 Parch와 SibSp의 합이다. 생존율이 승객의 가족 규모와 관련이 있는지 확인할 수 있도록 결합된 데이터를 제공한다. Alone은 승객이 혼자인지 여부를 나타낸다.","metadata":{}},{"cell_type":"code","source":"data['Family_Size']=0\ndata['Family_Size']=data['Parch']+data['SibSp']\ndata['Alone']=0\ndata.loc[data.Family_Size==0, 'Alone']=1\n\nf, ax = plt.subplots(1,2, figsize=(18,6))\nsns.pointplot('Family_Size','Survived', data=data, ax=ax[0])\nax[0].set_title('Family_Size vs Survived')\nsns.pointplot('Alone', 'Survived', data=data, ax=ax[1])\nax[1].set_title('Alone vs Survived')\nplt.close(2)\nplt.close(3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:22.269063Z","iopub.execute_input":"2022-02-03T12:43:22.269404Z","iopub.status.idle":"2022-02-03T12:43:22.843807Z","shell.execute_reply.started":"2022-02-03T12:43:22.269353Z","shell.execute_reply":"2022-02-03T12:43:22.842947Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"##### Family_Size=0 은 승객이 혼자인 걸 의미한다.\n* 분명히 혼자이거나 family_size=0이라면 생존 가능성은 매우 낮다. 가족규모가 5인이상인 경우도 줄어든다. 이것 또한 모델의 중요한 특성으로 보인다. 이것을 더 조사해보자.","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Alone', 'Survived', data=data, hue='Sex', col='Pclass')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:22.845065Z","iopub.execute_input":"2022-02-03T12:43:22.845302Z","iopub.status.idle":"2022-02-03T12:43:23.831320Z","shell.execute_reply.started":"2022-02-03T12:43:22.845274Z","shell.execute_reply":"2022-02-03T12:43:23.830514Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"* 혼자일 확률이 높은 Pclass3을 제외하고는 혼자 있는 여성이 가족이 있는 여성보다 성별이나 Pclass를 불문하고 혼자 있다는 것이 좋지 않다는 것을 알 수 있다.\n---\n### Fare_Range\n* 요금도 연속적인 특성이므로 서수값으로 변환해야 한다. 이를 위해 우리는 pandas.qcut을 사용할 것이다.\n* 따라서 qcut이 하는 일은 우리가 전달한 그룹의 수에 따라 값을 분할하거나 정렬하는 것이다. 따라서 5개의 그룹에 대해 전달하면 5개의 개별 그룹 또는 값 범위에 균등한 간격으로 값을 정렬한다.","metadata":{}},{"cell_type":"code","source":"data['Fare_Range']=pd.qcut(data['Fare'],4)\ndata.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:23.832638Z","iopub.execute_input":"2022-02-03T12:43:23.832859Z","iopub.status.idle":"2022-02-03T12:43:23.859549Z","shell.execute_reply.started":"2022-02-03T12:43:23.832831Z","shell.execute_reply":"2022-02-03T12:43:23.858602Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"* 위에서 논의한 바와 같이, 요금 범위가 증가함에 따라 생존 가능성이 증가한다는 것을 분명히 알 수 있다.\n* 이제 Fare_Range값을 그대로 전달할 수 없다. Age_Band에서 했던 것과 같은 싱글톤 값으로 변환해야 한다. ","metadata":{}},{"cell_type":"code","source":"data['Fare_cat']=0\ndata.loc[data['Fare']<=7.91,'Fare_cat']=0\ndata.loc[(data['Fare']>7.91)&(data['Fare']<=14.454), 'Fare_cat']=1\ndata.loc[(data['Fare']>14.454)&(data['Fare']<=31), 'Fare_cat']=2\ndata.loc[(data['Fare']>31)&(data['Fare']<=513), 'Fare_cat']=3","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:23.860978Z","iopub.execute_input":"2022-02-03T12:43:23.861204Z","iopub.status.idle":"2022-02-03T12:43:23.871400Z","shell.execute_reply.started":"2022-02-03T12:43:23.861178Z","shell.execute_reply":"2022-02-03T12:43:23.870465Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"sns.factorplot('Fare_cat', 'Survived', data=data, hue='Sex')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:23.872400Z","iopub.execute_input":"2022-02-03T12:43:23.873120Z","iopub.status.idle":"2022-02-03T12:43:24.439743Z","shell.execute_reply.started":"2022-02-03T12:43:23.873088Z","shell.execute_reply":"2022-02-03T12:43:24.438827Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"* 분명히 Fare_cat이 증가함에 따라 생존율이 증가한다. 이 특성은 Sex와 함께 모델링시 중요한 특성이 될 수 있다.\n---\n### Converting String Values into Numeric\n* 기계 학습 모델에 문자열을 전달할 수 없기 때문에 Sex, Embarked 등과 같은 기능을 숫자 값으로 변환해야 한다.","metadata":{}},{"cell_type":"code","source":"data['Sex'].replace(['male', 'female'],[0,1],inplace=True)\ndata['Embarked'].replace(['S','C','Q'],[0,1,2], inplace=True)\ndata['Initial'].replace(['Mr', 'Mrs','Miss','Master','Other'],[0,1,2,3,4], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:24.441171Z","iopub.execute_input":"2022-02-03T12:43:24.442122Z","iopub.status.idle":"2022-02-03T12:43:24.451584Z","shell.execute_reply.started":"2022-02-03T12:43:24.442081Z","shell.execute_reply":"2022-02-03T12:43:24.450894Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"#### 불필요한 특성 삭제\n* Name -> 범주형 값으로 변환할 수 없으므로 필요없다.\n* Age -> Age_band 특성이 있으므로 필요없다.\n* Ticket -> 분류할 수 없는 임의의 문자열이다.\n* Fare -> Fare_cat 특성이 있으므로 필요없다.\n* Cabin -> 많은 NaN값과 많은 승객이 여러 개의 객실을 가지고 있다. 따라서 쓸모없는 특성이다.\n* Fare_Range -> Fare_cat 특성이 있다.\n* Passengerid -> 분류할 수 없다.","metadata":{}},{"cell_type":"code","source":"data.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'], axis=1, inplace=True)\nsns.heatmap(data.corr(), annot=True, cmap='RdYlGn', linewidth=0.2, annot_kws={'size':20})\nfig=plt.gcf()\nfig.set_size_inches(18,15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:24.452720Z","iopub.execute_input":"2022-02-03T12:43:24.453564Z","iopub.status.idle":"2022-02-03T12:43:25.271794Z","shell.execute_reply.started":"2022-02-03T12:43:24.453514Z","shell.execute_reply":"2022-02-03T12:43:25.270833Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"* 이제 위의 상관 플롯에서 양성으로 관련된 몇 가지 특성을 볼 수 있다. 그들 중 일부는 SibSp와 Family_Size, Parch와 Family_size이고 일부는 Alone과 Family_Size 같은 음성으로 관련된 것이다.\n---\n## Part3 : Predictive Modeling\n* 우리는 EDA 부분에서 몇 가지 통찰력을 얻었다. 그러나 그것으로는 승객이 생존할지 사망할지 정확하게 예측하거나 말할 수 없다. 이제 우리는 훌륭한 분류 알고리즘을 사용하여 승객이 생존할지 여부를 예측할 것이다. 아래는 모델을 만드는데 사용할 알고리즘이다.\n    * Logistic Regression\n    * Support Vector Machines(Linear and Radial)\n    * Random Forest\n    * K-Nearest Neighbouts\n    * Naive Bayes\n    * Decision Tree","metadata":{}},{"cell_type":"code","source":"# importing all the required ML packages\nfrom sklearn.linear_model import LogisticRegression # logistic regression\nfrom sklearn import svm # support vector machine\nfrom sklearn.ensemble import RandomForestClassifier # Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier # KNN\nfrom sklearn.naive_bayes import GaussianNB # Naive Bayes\nfrom sklearn.tree import DecisionTreeClassifier # Decision Tree\nfrom sklearn.model_selection import train_test_split # training and testing data split\nfrom sklearn import metrics # accuracy measure\nfrom sklearn.metrics import confusion_matrix # for confusion matrix\n","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:25.273214Z","iopub.execute_input":"2022-02-03T12:43:25.273502Z","iopub.status.idle":"2022-02-03T12:43:25.828003Z","shell.execute_reply.started":"2022-02-03T12:43:25.273460Z","shell.execute_reply":"2022-02-03T12:43:25.827164Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"train, test=train_test_split(data, test_size=0.3, random_state=0, stratify=data['Survived'])\ntrain_X=train[train.columns[1:]]\ntrain_Y=train[train.columns[:1]]\ntest_X=test[test.columns[1:]]\ntest_Y=test[test.columns[:1]]\nX=data[data.columns[1:]]\nY=data['Survived']","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:25.829322Z","iopub.execute_input":"2022-02-03T12:43:25.829691Z","iopub.status.idle":"2022-02-03T12:43:25.844926Z","shell.execute_reply.started":"2022-02-03T12:43:25.829648Z","shell.execute_reply":"2022-02-03T12:43:25.844060Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"### Radial Support Vector Machines(rbf-SVM)","metadata":{}},{"cell_type":"code","source":"model=svm.SVC(kernel='rbf', C=1, gamma=0.1)\nmodel.fit(train_X, train_Y)\nprediction1=model.predict(test_X)\nprint('Accuracy for rbf SVM is ', metrics.accuracy_score(prediction1, test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:25.846123Z","iopub.execute_input":"2022-02-03T12:43:25.846422Z","iopub.status.idle":"2022-02-03T12:43:25.874396Z","shell.execute_reply.started":"2022-02-03T12:43:25.846381Z","shell.execute_reply":"2022-02-03T12:43:25.873664Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"### Linear Support Vector Machine(linear-SVM)","metadata":{}},{"cell_type":"code","source":"model=svm.SVC(kernel='linear', C=0.1, gamma=0.1)\nmodel.fit(train_X, train_Y)\nprediction2=model.predict(test_X)\nprint('Accuracy for linear SVM is', metrics.accuracy_score(prediction2, test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:25.875777Z","iopub.execute_input":"2022-02-03T12:43:25.876021Z","iopub.status.idle":"2022-02-03T12:43:25.893839Z","shell.execute_reply.started":"2022-02-03T12:43:25.875976Z","shell.execute_reply":"2022-02-03T12:43:25.893142Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"model=LogisticRegression()\nmodel.fit(train_X, train_Y)\nprediction3=model.predict(test_X)\nprint('The accuracy of the Logistic Regression is', metrics.accuracy_score(prediction3, test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:25.894936Z","iopub.execute_input":"2022-02-03T12:43:25.895252Z","iopub.status.idle":"2022-02-03T12:43:25.914471Z","shell.execute_reply.started":"2022-02-03T12:43:25.895225Z","shell.execute_reply":"2022-02-03T12:43:25.913919Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree","metadata":{}},{"cell_type":"code","source":"model=DecisionTreeClassifier()\nmodel.fit(train_X, train_Y)\nprediction4=model.predict(test_X)\nprint('The accuracy of the Decision Tree is', metrics.accuracy_score(prediction4, test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:25.915484Z","iopub.execute_input":"2022-02-03T12:43:25.915906Z","iopub.status.idle":"2022-02-03T12:43:25.928958Z","shell.execute_reply.started":"2022-02-03T12:43:25.915871Z","shell.execute_reply":"2022-02-03T12:43:25.927928Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"### K-Nearest Neighbours(KNN)","metadata":{}},{"cell_type":"code","source":"model=KNeighborsClassifier()\nmodel.fit(train_X, train_Y)\nprediction5=model.predict(test_X)\nprint('The accuracy of the KNN is', metrics.accuracy_score(prediction5, test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:25.930180Z","iopub.execute_input":"2022-02-03T12:43:25.930727Z","iopub.status.idle":"2022-02-03T12:43:25.961710Z","shell.execute_reply.started":"2022-02-03T12:43:25.930686Z","shell.execute_reply":"2022-02-03T12:43:25.960776Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"* 이제 n_neighbours 속성의 값을 변경함에 따라 KNN 모델의 정확도가 변한다. 기본값은 5이다. 다양한 n_neighbours값에 대한 정확도를 확인하자.","metadata":{}},{"cell_type":"code","source":"a_index=list(range(1,11))\na=pd.Series()\nx=[0,1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    model=KNeighborsClassifier(n_neighbors=i)\n    model.fit(train_X, train_Y)\n    prediction=model.predict(test_X)\n    a=a.append(pd.Series(metrics.accuracy_score(prediction, test_Y)))\nplt.plot(a_index, a)\nplt.xticks(x)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()\nprint('Accuracies for different values of n are:', a.values, 'with the max value as ', a.values.max())","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:25.962819Z","iopub.execute_input":"2022-02-03T12:43:25.963030Z","iopub.status.idle":"2022-02-03T12:43:26.327004Z","shell.execute_reply.started":"2022-02-03T12:43:25.963005Z","shell.execute_reply":"2022-02-03T12:43:26.326111Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"### Gaussian Naive Bayes","metadata":{}},{"cell_type":"code","source":"model=GaussianNB()\nmodel.fit(train_X, train_Y)\nprediction6=model.predict(test_X)\nprint('The accuracy of the NaiveBayes is', metrics.accuracy_score(prediction6, test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:26.328156Z","iopub.execute_input":"2022-02-03T12:43:26.328959Z","iopub.status.idle":"2022-02-03T12:43:26.339698Z","shell.execute_reply.started":"2022-02-03T12:43:26.328891Z","shell.execute_reply":"2022-02-03T12:43:26.338987Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"### Random Forests","metadata":{}},{"cell_type":"code","source":"model=RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X, train_Y)\nprediction7=model.predict(test_X)\nprint('The accuracy of the Random Forests is', metrics.accuracy_score(prediction7, test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:26.340840Z","iopub.execute_input":"2022-02-03T12:43:26.341188Z","iopub.status.idle":"2022-02-03T12:43:26.533648Z","shell.execute_reply.started":"2022-02-03T12:43:26.341159Z","shell.execute_reply":"2022-02-03T12:43:26.533037Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"* 모델의 정확도가 분류기의 견고성을 결정하는 유일한 요소는 아니다. 분류기가 훈련 데이터에 대해 훈련되고 테스트 데이터에 대해 테스트되었으며 90%의 정확도를 기록했다고 가정해보자!\n* 이제 분류기의 정확도가 매우 좋은 것 같지만, 모든 새로운 테스트 세트에 대해 90%의 정확도를 확인할 수 있을까요? 대답은 아니오이다. 분류기가 자체 학습에 사용할 모든 인스턴스를 결정할 수 없기 때문이다. 훈련 및 테스트 데이터가 변경되면 정확도도 변경된다. 증가하거나 감소할 수 있다. 이것을 모델 분산이라고 한다.\n* 이를 극복하고 일반화된 모델을 얻기 위해 Cross Validation(교차검증)을 사용한다.\n---\n### Cross Validation\n* 대다수의 경우 데이터가 불균형하다. 즉, class1 인스턴스의 수는 많지만 다른 class 인스턴스의 수는 적을 수 있다. 따라서 데이터 세트의 모든 인스턴스에 대해 알고리즘을 훈련하고 테스트해야 한다. 그런 다음 데이터 세트에 대해 언급된 모든 정확도의 평균을 취할 수 있다.   \n* 1) K-Fold 교차 검증은 먼저 데이터 세트를 k-subsets으로 나누어 작동한다.      \n* 2) 데이터 세트를 k=5 로  나눈다. 테스트를 위해 1개를 놔두고 4개는 알고리즘을 훈련한다.       \n* 3) 각 반복마다 테스트 파트를 변경하고 다른 파트에 대해 알고리즘을 훈련하여 프로세스를 계속한다. 그런 다음 정확도와 오류를 평균하여 알고리즘의 평균 정확도를 얻는다.       \n* 이것을 K-Fold 교차 검증이라고 한다.\n* 4) 알고리즘은 일부 훈련 데이터에 대한 데이터 세트에 대해 과소적합될 수 있고 때로는 다른 훈련 세트에 대한 데이터에 과대적합될 수도 있다. 따라서 교차검증을 통해 일반화된 모델을 얻을 수 있다.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold # for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score # score evaluation\nfrom sklearn.model_selection import cross_val_predict # prediction\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nxyz = []\naccuracy = []\nstd = []\nclassifiers=['Linear Svm', 'Radial Svm', 'Logistic Regression', 'KNN', 'Decision Tree','Naive Bayes', 'Random Forest']\nmodels=[svm.SVC(kernel='linear'), svm.SVC(kernel='rbf'), \n        LogisticRegression(), KNeighborsClassifier(n_neighbors=9),\n        DecisionTreeClassifier(), GaussianNB(), RandomForestClassifier(n_estimators=100)]\nfor i in models:\n    model = i\n    cv_result=cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')\n    cv_result=cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame({'CV Mean':xyz, 'Std':std}, index=classifiers)\nnew_models_dataframe2","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:26.534684Z","iopub.execute_input":"2022-02-03T12:43:26.535021Z","iopub.status.idle":"2022-02-03T12:43:29.133845Z","shell.execute_reply.started":"2022-02-03T12:43:26.534994Z","shell.execute_reply":"2022-02-03T12:43:29.132894Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"plt.subplots(figsize=(12,6))\nbox=pd.DataFrame(accuracy, index=[classifiers])\nbox.T.boxplot()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:29.135322Z","iopub.execute_input":"2022-02-03T12:43:29.135589Z","iopub.status.idle":"2022-02-03T12:43:29.419796Z","shell.execute_reply.started":"2022-02-03T12:43:29.135559Z","shell.execute_reply":"2022-02-03T12:43:29.418944Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"new_models_dataframe2['CV Mean'].plot.barh(width=0.8)\nplt.title('Average CV Mean Accuracy')\nfig=plt.gcf()\nfig.set_size_inches(8,5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:29.420954Z","iopub.execute_input":"2022-02-03T12:43:29.421185Z","iopub.status.idle":"2022-02-03T12:43:29.630065Z","shell.execute_reply.started":"2022-02-03T12:43:29.421150Z","shell.execute_reply":"2022-02-03T12:43:29.629144Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"* 분류 정확도는 불균형으로 인해 때때로 오해의 소지가 있다. 모델이 어디에서 잘못되었는지 또는 모델이 어떤 클래스를 잘못 예측했는지 보여주는 혼동 행렬의 도움으로 요약된 결과를 얻을 수 있다. \n---\n### Confusion Matrix(혼동행렬)\n* 분류기가 만든 옳고 그른 분류의 수를 제공한다.","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(3,3,figsize=(12,10))\ny_pred = cross_val_predict(svm.SVC(kernel='rbf'),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,0],annot=True,fmt='2.0f')\nax[0,0].set_title('Matrix for rbf-SVM')\ny_pred = cross_val_predict(svm.SVC(kernel='linear'),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,1],annot=True,fmt='2.0f')\nax[0,1].set_title('Matrix for Linear-SVM')\ny_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,2],annot=True,fmt='2.0f')\nax[0,2].set_title('Matrix for KNN')\ny_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,0],annot=True,fmt='2.0f')\nax[1,0].set_title('Matrix for Random-Forests')\ny_pred = cross_val_predict(LogisticRegression(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,1],annot=True,fmt='2.0f')\nax[1,1].set_title('Matrix for Logistic Regression')\ny_pred = cross_val_predict(DecisionTreeClassifier(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,2],annot=True,fmt='2.0f')\nax[1,2].set_title('Matrix for Decision Tree')\ny_pred = cross_val_predict(GaussianNB(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[2,0],annot=True,fmt='2.0f')\nax[2,0].set_title('Matrix for Naive Bayes')\nplt.subplots_adjust(hspace=0.2, wspace=0.2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:29.631353Z","iopub.execute_input":"2022-02-03T12:43:29.631665Z","iopub.status.idle":"2022-02-03T12:43:34.347283Z","shell.execute_reply.started":"2022-02-03T12:43:29.631604Z","shell.execute_reply":"2022-02-03T12:43:34.346358Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"#### Interpreting Confusion Matrix\n* 왼쪽 대각선은 각 클래스에 대해 수행된 올바른 예측의 수를 나타내고 오른쪽 대각선은 잘못된 예측의 수를 나타낸다. rbf-SVM의 첫 번째 플롯을 고려해보자.\n* 정확한 예측수는 491(사망) + 247(생존)이며 평균 CV정확도는 (491+247)/891 = 82.8% 이다.\n* 오류 -> 58명의 사망자를 생존자로, 95명이 사망한 것으로 잘 못 분류했다. 따라서 죽은 사람을 생존자로 예측함으로써 더 많은 실수를 저질렀다.\n* 모든 행렬을 살펴보면 rbf-SVM이 사망한 승객을 정확하게 예측할 확률이 더 높지만 NaiveBayes가 생존한 승객을 정확하게 예측할 확룰이 더 높다고 말할 수 있다.\n---\n#### Hyper-Parameters Tuning\n* 기계 학습 모델은 블랙박스와 같다. 이 블랙박스에 대한 몇가지 기본 매개변수 값이 있으며 더 나은 모델을 얻기 위해 조정하거나 변경할 수 있다. SVM 모델의 C 및 gamma와 마찬가지로 분류기에 대한 유사하게 다른 매개변수를 하이퍼 파라미터라고 하며 알고리즘의 학습률을 변경하고 더 나은 모델을 얻기 위해 조정할 수 있다. 이것을 하이퍼파라미터 튜닝이라고 한다.\n* 우리는 2개의 최고의 분류기, 즉 SVM과 RandomForests에 대한 하이퍼파라미터를 조정할 것이다.\n---\n### SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nC=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\ngamma=[-.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nkernel=['rbf','linear']\nhyper={'kernel':kernel, 'C':C, 'gamma':gamma}\ngd=GridSearchCV(estimator=svm.SVC(), param_grid=hyper, verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:34.348451Z","iopub.execute_input":"2022-02-03T12:43:34.348755Z","iopub.status.idle":"2022-02-03T12:43:53.652202Z","shell.execute_reply.started":"2022-02-03T12:43:34.348720Z","shell.execute_reply":"2022-02-03T12:43:53.651312Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"### Random Forests","metadata":{}},{"cell_type":"code","source":"n_estimators=range(100,1000,100)\nhyper={'n_estimators':n_estimators}\ngd=GridSearchCV(estimator=RandomForestClassifier(random_state=0), param_grid=hyper, verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:43:53.653451Z","iopub.execute_input":"2022-02-03T12:43:53.653982Z","iopub.status.idle":"2022-02-03T12:44:32.630069Z","shell.execute_reply.started":"2022-02-03T12:43:53.653937Z","shell.execute_reply":"2022-02-03T12:44:32.629110Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"* Rbf-SVM에 대한 최고 점수는 C=0.4 및 gamma=0.3 일 때 82.82%이다.\n* RandomForest의 경우 점수는 n_estimators=300일 때 약 81.9%이다.\n---\n## Ensembling\n* 앙상블은 모델의 정확도나 성능을 높이는 좋은 방법입니다. 간단히 말해서 여러 가지 단순한 모델을 조합하여 하나의 강력한 모델을 만드는 것입니다.\n* 우리가 전화기를 구매하고 싶고 다양한 매개변수를 기반으로 많은 사람들에게 전화기 정보에 대해 물어보고 싶다고 가정해 보자. 따라서 다양한 매개변수를 모두 분석한 후 단일 제품에 대해 강력한 판단을 내릴 수 있다. 모델의 안정성을 향상시키는 이것이 앙상블이다. 앙상블은 다음과 같은 방법으로 수행할 수 있다. :   \n1) Voting Classifier  \n2) Bagging   \n3) Boosting  \n---\n### Voting Classifier\n* 다양한 단순 기계학습모델의 예측을 결합하는 가장 간단한 방법이다. 모든 하위 모델의 예측을 기반으로 평균 예측결과를 제공한다. 하위 모델 또는 기본 모델은 모두 다른 유형이다.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nensemble_lin_rbf =VotingClassifier(estimators=[('KNN', KNeighborsClassifier(n_neighbors=10)),\n                                              ('RBF', svm.SVC(probability=True,kernel='rbf', C=0.5,gamma=0.1)),\n                                              ('RFor', RandomForestClassifier(n_estimators=500, random_state=0)),\n                                              ('LR', LogisticRegression(C=0.05)),\n                                              ('DT', DecisionTreeClassifier(random_state=0)),\n                                              ('NB', GaussianNB()),\n                                              ('svm', svm.SVC(kernel='linear', probability=True))], voting='soft').fit(train_X, train_Y)\nprint('The accuracy for ensembled model is:', ensemble_lin_rbf.score(test_X, test_Y))\ncross=cross_val_score(ensemble_lin_rbf, X, Y, cv=10, scoring='accuracy')\nprint('The cross validated score is', cross.mean())","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:44:32.631506Z","iopub.execute_input":"2022-02-03T12:44:32.632183Z","iopub.status.idle":"2022-02-03T12:44:44.353551Z","shell.execute_reply.started":"2022-02-03T12:44:32.632144Z","shell.execute_reply":"2022-02-03T12:44:44.352706Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"### Bagging\n* Bagging은 일반적인 앙상블 방법이다. 데이터 세트의 작은 파티션에 유사한 분류기를 적용한 다음 모든 예측의 평균을 취하여 작동한다. 평균화로 인해 분산이 감소한다. Voting Classifier와 달리 Bagging은 유사한 classifier를 사용한다.\n---\n#### Bagged KNN\n* Bagging은 분산이 높은 모델에서 가장 잘 작동한다. 이에 대한 예로는 Decision Tree or Random Forest가 있다. n_neighbors의 작은 값으로 KNN을 n_neighbors의 작은 값으로 사용할 수 있다.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nmodel=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3), random_state=0, n_estimators=700)\nmodel.fit(train_X, train_Y)\nprediction=model.predict(test_X)\nprint('The accuracy for bagged KNN is:', metrics.accuracy_score(prediction, test_Y))\nresult=cross_val_score(model, X, Y, cv=10, scoring='accuracy')\nprint('The cross validated score for bagged KNN is:', result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:44:44.354799Z","iopub.execute_input":"2022-02-03T12:44:44.355458Z","iopub.status.idle":"2022-02-03T12:45:10.335232Z","shell.execute_reply.started":"2022-02-03T12:44:44.355412Z","shell.execute_reply":"2022-02-03T12:45:10.334323Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"#### Bagged DecisionTree","metadata":{}},{"cell_type":"code","source":"model=BaggingClassifier(base_estimator=DecisionTreeClassifier(), random_state=0, n_estimators=100)\nmodel.fit(train_X, train_Y)\nprediction=model.predict(test_X)\nprint('The accuracy for bagged Decision Tree is:', metrics.accuracy_score(prediction, test_Y))\nresult=cross_val_score(model, X, Y, cv=10, scoring='accuracy')\nprint('The cross validated score for bagged Decision Tree is:', result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:45:10.336567Z","iopub.execute_input":"2022-02-03T12:45:10.336909Z","iopub.status.idle":"2022-02-03T12:45:13.059522Z","shell.execute_reply.started":"2022-02-03T12:45:10.336865Z","shell.execute_reply":"2022-02-03T12:45:13.058400Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"#### Boosting\n* Boosting은 classifier의 순차적 학습을 사용하는 앙상블 기술이다. 약한 모델을 단계적으로 개선하는 것이다. 부스팅은 다음과 같이 작동한다.\n* 모델은 먼저 전체 데이터 세트에서 학습된다. 이제 모델은 일부 인스턴스는 맞고 일부는 틀리게 된다. 다음 반복에서는 학습모델은 잘못 예측된 인스턴스에 더 집중하거나 더 많은 가중치를 부여한다. 따라서 잘못된 인스턴스를 올바르게 예측하려고 시도한다. 이제 이 반복 프로세스가 계속 되고 정확도의 한계에 도달할 때까지 새로운 classifier가 모델에 추가된다.\n---\n##### AdaBoost(Adaptive Boosting)\n* 이 경우 약한 학습모델 또는 추정기는 Decision Tree이다. 그러나 우리는 default base_estimator를 우리가 선택한 알고리즘으로 변경할 수 있다.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nada=AdaBoostClassifier(n_estimators=200, random_state=0, learning_rate=0.1)\nresult=cross_val_score(ada, X,Y,cv=10, scoring='accuracy')\nprint('The cross validated score for AdaBoost is:', result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:45:13.060796Z","iopub.execute_input":"2022-02-03T12:45:13.061087Z","iopub.status.idle":"2022-02-03T12:45:16.655243Z","shell.execute_reply.started":"2022-02-03T12:45:13.061046Z","shell.execute_reply":"2022-02-03T12:45:16.654232Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"##### Stochastic Gradient Boosting\n* 여기서도 약한 학습모델은 Decision Tree이다.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngrad=GradientBoostingClassifier(n_estimators=500, random_state=0, learning_rate=0.1)\nresult=cross_val_score(grad, X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for Gradient Boosting is:', result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:45:16.656354Z","iopub.execute_input":"2022-02-03T12:45:16.656583Z","iopub.status.idle":"2022-02-03T12:45:21.473806Z","shell.execute_reply.started":"2022-02-03T12:45:16.656555Z","shell.execute_reply":"2022-02-03T12:45:21.473248Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"##### XGBoost","metadata":{}},{"cell_type":"code","source":"import xgboost as xg\nxg.set_config(verbosity=0) # 이게 뭘까??\nxgboost=xg.XGBClassifier(n_estimators=900, learning_rate=0.1)\nresult=cross_val_score(xgboost,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for XGBoost is:', result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:45:21.475275Z","iopub.execute_input":"2022-02-03T12:45:21.476374Z","iopub.status.idle":"2022-02-03T12:45:48.222056Z","shell.execute_reply.started":"2022-02-03T12:45:21.476328Z","shell.execute_reply":"2022-02-03T12:45:48.220842Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"* AdaBoost에서 가장 높은 정확도를 얻었다. 하이퍼파라미터 튜닝으로 높여보도록 하자.\n---\n##### Hyper-Parameter Tuning for AdaBoost","metadata":{}},{"cell_type":"code","source":"n_estimators=list(range(100,1100,100))\nlearn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\nhyper={'n_estimators':n_estimators, 'learning_rate':learn_rate}\ngd=GridSearchCV(estimator=AdaBoostClassifier(), param_grid=hyper, verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:45:48.223247Z","iopub.execute_input":"2022-02-03T12:45:48.223478Z","iopub.status.idle":"2022-02-03T12:55:28.400943Z","shell.execute_reply.started":"2022-02-03T12:45:48.223449Z","shell.execute_reply":"2022-02-03T12:55:28.399769Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"* AdaBoost로 얻을 수 있는 최대 정확도는 83.16%이다. 이때 n_estimator= 이고 learning_rate= 이다.\n---\n### Confusion Matrix for the Best Model","metadata":{}},{"cell_type":"code","source":"ada=AdaBoostClassifier(n_estimators=200, random_state=0, learning_rate=0.05)\nresult=cross_val_predict(ada,X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,result),cmap='winter',annot=True,fmt='2.0f')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:55:28.402445Z","iopub.execute_input":"2022-02-03T12:55:28.402681Z","iopub.status.idle":"2022-02-03T12:55:32.179246Z","shell.execute_reply.started":"2022-02-03T12:55:28.402655Z","shell.execute_reply":"2022-02-03T12:55:32.178291Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(2,2,figsize=(15,12))\nmodel=RandomForestClassifier(n_estimators=500,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])\nax[0,0].set_title('Feature Importance in Random Forests')\nmodel=AdaBoostClassifier(n_estimators=200,learning_rate=0.05,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='#ddff11')\nax[0,1].set_title('Feature Importance in AdaBoost')\nmodel=GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')\nax[1,0].set_title('Feature Importance in Gradient Boosting')\nmodel=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')\nax[1,1].set_title('Feature Importance in XgBoost')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:55:32.180708Z","iopub.execute_input":"2022-02-03T12:55:32.180980Z","iopub.status.idle":"2022-02-03T12:55:37.157040Z","shell.execute_reply.started":"2022-02-03T12:55:32.180953Z","shell.execute_reply":"2022-02-03T12:55:37.156172Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"* RandomForest, AdaBoost등과 같은 다양한 분류기의 중요한 특성들을 볼 수 있다.\n----\n##### Observations:   \n1) 일반적인 중요한 특성 중 일부는 Initial, Fare_cat, Pclass, Family_Size이다.   \n2) Sex 특성은 중요도를 부여하지 않는 것 같다. 이전에 Pclass와 결합된 Sex가 매우 좋은 차별화 요소를 제공하는 것을 보았듯이 충격적이다. Sex는 RandomForests에서만 중요한 것 같다. 그러나 많은 분류기에서 맨 위에 있는 Initial 특성을 볼 수 있다. 우리는 이미 Sex와 Initial 사이의 양의 상관관계를 보았으므로 둘 다 성별을 나타낸다.   \n3) 유사하게 Pclass 및 Fare_cat은 Alone, Parch 및 SibSp와 함께 승객 및 Family_Size의 상태를 나타낸다.","metadata":{}}]}